{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"main.ipynb","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q petals datasets","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:11:55.534747Z","iopub.execute_input":"2023-02-27T17:11:55.535104Z","iopub.status.idle":"2023-02-27T17:12:05.842498Z","shell.execute_reply.started":"2023-02-27T17:11:55.535071Z","shell.execute_reply":"2023-02-27T17:12:05.841281Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Базовые импорты\nimport os\n\nimport torch\nfrom transformers import BloomTokenizerFast\nfrom petals import DistributedBloomForCausalLM\nfrom tqdm import tqdm\nimport json\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:12:05.845445Z","iopub.execute_input":"2023-02-27T17:12:05.846398Z","iopub.status.idle":"2023-02-27T17:12:16.402826Z","shell.execute_reply.started":"2023-02-27T17:12:05.846365Z","shell.execute_reply":"2023-02-27T17:12:16.401676Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Попробуем сгенерировать текст при помощи модели","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"bigscience/bloom-petals\"\nTUNING_MODE = 'ptune'\nNUM_PREFIX_TOKENS = 16\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nSEED = 42\nMODEL_MAX_LENGTH = 256","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:12:16.404568Z","iopub.execute_input":"2023-02-27T17:12:16.404960Z","iopub.status.idle":"2023-02-27T17:12:16.412980Z","shell.execute_reply.started":"2023-02-27T17:12:16.404922Z","shell.execute_reply":"2023-02-27T17:12:16.411070Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"tokenizer = BloomTokenizerFast.from_pretrained(MODEL_NAME)\ntokenizer.padding_side = 'right'\ntokenizer.model_max_length = MODEL_MAX_LENGTH\nmodel = DistributedBloomForCausalLM.from_pretrained(\n    MODEL_NAME,\n    pre_seq_len=NUM_PREFIX_TOKENS, \n    tuning_mode=TUNING_MODE,\n    request_timeout=1000\n).to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:12:16.415026Z","iopub.execute_input":"2023-02-27T17:12:16.415918Z","iopub.status.idle":"2023-02-27T17:16:30.431310Z","shell.execute_reply.started":"2023-02-27T17:12:16.415879Z","shell.execute_reply":"2023-02-27T17:16:30.428424Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/263 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3178454ddb5f41f88a240587ede40b4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20b82807fb864cb09f1a84d8bf5f33d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad2767870bc743fc8c175178b00f334b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/641 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0158c40fc83146b48fc0f18764ff375c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/7.19G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"121760d0bc934f41ba2118bc49e1e11e"}},"metadata":{}}]},{"cell_type":"code","source":"TOP_K = 100\nTEMPERATURE = 0.6\nuser_phrase = 'Привет, как твои дела?\\n'\ninputs = tokenizer(user_phrase, return_tensors='pt')['input_ids'].to(DEVICE)\noutputs = model.generate(\n                inputs,\n                temperature=TEMPERATURE,\n                do_sample=True,\n                top_k=TOP_K,\n                max_new_tokens=8,\n            )\nbloom_answer_token = tokenizer.decode(outputs[0])\nprint(bloom_answer_token)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:16:30.434304Z","iopub.execute_input":"2023-02-27T17:16:30.435058Z","iopub.status.idle":"2023-02-27T17:17:33.735998Z","shell.execute_reply.started":"2023-02-27T17:16:30.435018Z","shell.execute_reply":"2023-02-27T17:17:33.734743Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Feb 27 17:16:36.183 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1m/opt/conda/lib/python3.7/site-packages/petals/client/inference_session.py.step:312\u001b[0m] Caught exception when running inference from block 0 (retry in 0 sec): ControlFailure('Connect failed. msg=failed to find peers: routing: not found')\nFeb 27 17:16:36.515 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1m/opt/conda/lib/python3.7/site-packages/petals/client/inference_session.py.step:312\u001b[0m] Caught exception when running inference from block 0 (retry in 1 sec): ControlFailure('Connect failed. msg=failed to find peers: routing: not found')\n","output_type":"stream"},{"name":"stdout","text":"Привет, как твои дела?\nЗдравствуй, как\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Отлично, не без проблем, но с моделью установлен контакт, можно переходить к следующему этапу.","metadata":{}},{"cell_type":"markdown","source":"## Теперь посмотрим на датасет, с которым предлагается поэкспериментировать","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/chain-of-thoughts/test.jsonl') as file:\n    data = [json.loads(line) for line in file.readlines() if line]\ndata[:3]","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:17:33.737776Z","iopub.execute_input":"2023-02-27T17:17:33.738172Z","iopub.status.idle":"2023-02-27T17:17:33.775082Z","shell.execute_reply.started":"2023-02-27T17:17:33.738135Z","shell.execute_reply":"2023-02-27T17:17:33.774029Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[{'question': \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n  'answer': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18'},\n {'question': 'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?',\n  'answer': 'It takes 2/2=<<2/2=1>>1 bolt of white fiber\\nSo the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric\\n#### 3'},\n {'question': 'Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?',\n  'answer': 'The cost of the house and repairs came out to 80,000+50,000=$<<80000+50000=130000>>130,000\\nHe increased the value of the house by 80,000*1.5=<<80000*1.5=120000>>120,000\\nSo the new value of the house is 120,000+80,000=$<<120000+80000=200000>>200,000\\nSo he made a profit of 200,000-130,000=$<<200000-130000=70000>>70,000\\n#### 70000'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"Как мы видим, предложенный датасет состоит из списка словарей с вопросом и развернутым ответом. Отметим, что в ответах численные выражения отдельно выделены в <<>>, а также правильный ответ дополнительно помещен в конец строки после ####. Насколько я понял, ответы на все вопросы в GSM8K представляют собой целые числа, что немного облегачает задачу.","metadata":{}},{"cell_type":"markdown","source":"## Перейдем к эксперименту, сперва поймем что необходимо делать и что мы ожидаем","metadata":{"execution":{"iopub.status.busy":"2023-02-19T18:23:51.008337Z","iopub.execute_input":"2023-02-19T18:23:51.008726Z","iopub.status.idle":"2023-02-19T18:23:51.013928Z","shell.execute_reply.started":"2023-02-19T18:23:51.008687Z","shell.execute_reply":"2023-02-19T18:23:51.012777Z"}}},{"cell_type":"markdown","source":"Отметим, что в статье https://arxiv.org/pdf/2203.11171.pdf (SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS) есть аналогичные эксперименты с моделью GPT-3 c 175B параметров, что близко к рассматриваемой нами BLOOM-176B. Причем есть табличные результаты и подтверждающие их json-файлы на том же датасете GSM8K, так что нам есть на что ориентироваться по качеству. \n\nОсновная рассматриваемая метрика - это Accuracy, то есть доля правильных ответов.\n\nТакже возьмем параметры сэмплирования из статьи, температура T=0.7, top-k с k=40. (Замечу, что для GPT-3 в работе отсечение по top-k не использовали совсем, но для BLOOM его лучше осуществлять). Далее данные параметры можно будет потюнить, чтобы попробовать получить эффект робастности из статьи.","metadata":{}},{"cell_type":"markdown","source":"Теперь натравим языковую модель на арифметические задачи из GSM8K и измерим accuracy в случае жадного CoT и self-consistency CoT.","metadata":{}},{"cell_type":"code","source":"PROMT=\"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n\\n\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n\\n\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\n\\n\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\nA: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\n\\n\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\nA: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\n\\n\nQ: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\nA: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\n\\n\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\nA: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\n\\n\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\n\\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:17:33.778728Z","iopub.execute_input":"2023-02-27T17:17:33.779011Z","iopub.status.idle":"2023-02-27T17:17:33.785811Z","shell.execute_reply.started":"2023-02-27T17:17:33.778985Z","shell.execute_reply":"2023-02-27T17:17:33.784769Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"В качестве промта воспользуемся предложенными авторами https://arxiv.org/pdf/2201.11903.pdf (Chain-of-Thought Prompting Elicits Reasoning in Large Language Models) для арифметических задач. К тому же их же использовали в статье про self-consistency, так что сравнению с полученными там результатами будет более корректно. В конце каждого ответа в промте есть фраза \"The answer is ...\", что должно помочь языковой модели выдавать ответ.","metadata":{"execution":{"iopub.status.busy":"2023-02-19T21:48:33.924547Z","iopub.execute_input":"2023-02-19T21:48:33.924916Z","iopub.status.idle":"2023-02-19T21:48:33.931020Z","shell.execute_reply.started":"2023-02-19T21:48:33.924883Z","shell.execute_reply":"2023-02-19T21:48:33.930036Z"}}},{"cell_type":"markdown","source":"Посмотрим на то, как справится модель с одним вопросом из датасета.","metadata":{}},{"cell_type":"code","source":"q = \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\ntext = PROMT + 'Q: ' + q + '\\nA: '\ninputs = tokenizer(text, return_tensors='pt')['input_ids'].to(DEVICE)\nstop_token_ids = tokenizer(tokenizer.eos_token)['input_ids']\noutputs = model.generate(inputs, max_new_tokens=100, temperature=0.7, top_k=40, do_sample=True, stop_token_ids=stop_token_ids)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:27:08.000207Z","iopub.execute_input":"2023-02-27T17:27:08.000570Z","iopub.status.idle":"2023-02-27T17:38:33.041741Z","shell.execute_reply.started":"2023-02-27T17:27:08.000538Z","shell.execute_reply":"2023-02-27T17:38:33.040663Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Feb 27 17:27:08.733 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1m/opt/conda/lib/python3.7/site-packages/petals/client/inference_session.py.step:312\u001b[0m] Caught exception when running inference from block 0 (retry in 0 sec): ControlFailure('Connect failed. msg=failed to find peers: routing: not found')\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.decode(outputs[0])","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:38:33.044120Z","iopub.execute_input":"2023-02-27T17:38:33.044507Z","iopub.status.idle":"2023-02-27T17:38:33.084544Z","shell.execute_reply.started":"2023-02-27T17:38:33.044466Z","shell.execute_reply":"2023-02-27T17:38:33.083528Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\n\\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\nA: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\\n\\n\\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\nA: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\\n\\n\\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\nA: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\\n\\n\\nQ: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\nA: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\\n\\n\\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\nA: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\\n\\n\\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\nA: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\\n\\n\\nQ: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\\nA:  She has 16 eggs per day. She eats 3 for breakfast. So she is left with 13 eggs. She bakes muffins with 4 eggs. So she is left with 9 eggs. Then she sells 9 eggs at $2 each. The answer is $2 * 9 = 18.\\n\\n\\nQ: The number of people at the movie theater is the square root of 36. The number of seats in the theater is the square root of 40. How many people are in the movie theater?\\nA: The\""},"metadata":{}}]},{"cell_type":"markdown","source":"Стоит отметить, что ответа пришлось ждать порядка 10 минут. Так что придется работать с небольшим количеством примеров.","metadata":{}},{"cell_type":"markdown","source":"# Вспомогательные функции\nИспользуются для того, чтобы получить ответ из предсказания нейросети и для агрегации нескольких ответов.","metadata":{}},{"cell_type":"code","source":"import re\nNUMBER_SET = [str(num) for num in range(0, 10)]\n\ndef _is_float(s):\n    try:\n        float(s)\n        return True\n    except:\n        return False\n\nFINAL_ANS = 'answer is '\ndef clean_ans(ans):\n    index = ans.find('.')\n    if index >= 0:\n        end_index = index + 1\n        while end_index < len(ans) and ans[end_index] in NUMBER_SET:\n            end_index += 1\n        ans = ans[:end_index]\n    while ans and ans.endswith('.'):\n        ans = ans[:-1]\n  \n    ans = ans.split('=')[-1].strip()\n    for c in ['$', ',', '%', '€', '\"']:\n        ans = ans.replace(c, '')\n    parts = ans.split(' ')\n    for part in parts:\n        if _is_float(part):\n            return part\n  \n    ans = parts[0]  # default\n    for part in parts:\n        if not part.isalpha():  # take the 1st non-alpha token\n            ans = part\n            break\n    while ans and ans[-1].isalpha():\n        ans = ans[:-1]\n    return ans.strip()\n    \ndef get_ans(pred):\n    text = pred.split('Q:')[0].split('[eot]')[0].replace('\\n', '').strip()\n    if text.rfind(FINAL_ANS) >= 0:\n        pred_ans = text[text.rfind(FINAL_ANS) + len(FINAL_ANS):len(text)].strip()\n        return clean_ans(pred_ans)\n    else:\n        return ''\n\n\nfrom collections import Counter\ndef get_maj(ans_list):\n    is_all_float = True\n    float_list = []\n    for ans in ans_list:\n        if _is_float(ans):\n            float_list.append(float(ans))\n        else:\n            is_all_float = False\n            break\n    if is_all_float:\n        f = Counter(float_list)\n        return f.most_common()[0][0]\n    else:\n        c = Counter(ans_list)\n        return c.most_common()[0][0]\n\ndef get_str_ans(pred):\n    text = pred.split('Q:')[0].split('[eot]')[0].replace('\\n', '').strip()\n    if text.rfind(FINAL_ANS) >= 0:\n        pred_ans = text[text.rfind(FINAL_ANS) + len(FINAL_ANS):len(text)].strip()\n        if pred_ans.endswith('.'):\n            pred_ans = pred_ans[:-1]\n        return pred_ans\n    else:\n        return ''","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:40:04.648811Z","iopub.execute_input":"2023-02-27T17:40:04.649188Z","iopub.status.idle":"2023-02-27T17:40:04.667125Z","shell.execute_reply.started":"2023-02-27T17:40:04.649156Z","shell.execute_reply":"2023-02-27T17:40:04.666080Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Так как к сожалению доступ по публичному API к языковой модели занимает очень много времени - придется в эксперименте сильно порезать количество решаемых задач. Чтобы исключить элемент случайности, насэмплируем случайные из датасета.","metadata":{}},{"cell_type":"code","source":"import random\n\nnum_tasks = 20\n\ninds = random.sample(range(0, len(data)), num_tasks)\ndata_test = [data[i] for i in inds]","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:42:01.754346Z","iopub.execute_input":"2023-02-27T17:42:01.754741Z","iopub.status.idle":"2023-02-27T17:42:01.760932Z","shell.execute_reply.started":"2023-02-27T17:42:01.754686Z","shell.execute_reply":"2023-02-27T17:42:01.759816Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Здесь представлены основные две функции, для получения предсказания и подсчета accuracy на данном предсказании.","metadata":{}},{"cell_type":"code","source":"def get_predictions(data_test, PROMT, examples=False, max_tokens=100, self_cons=False, num_samples=10, temp=0.7, top_k=40):\n    result = []\n    for elem in tqdm(data_test):\n        q = elem['question']\n        a = elem['answer']\n        \n        text = PROMT + '\\nQ: ' + q + '\\nA: '\n        inputs = tokenizer(text, return_tensors='pt')['input_ids'].to(DEVICE)\n        stop_token_ids = tokenizer(tokenizer.eos_token)['input_ids']\n        \n        ans = []\n        if self_cons:\n            for _ in tqdm(range(num_samples)):\n                outputs = model.generate(inputs, max_new_tokens=max_tokens, temperature=temp, top_k=top_k, do_samples=True, stop_token_ids=stop_token_ids)\n                ans.append(tokenizer.decode(outputs[0]))\n        else:\n            outputs = model.generate(inputs, max_new_tokens=max_tokens)\n            ans.append(tokenizer.decode(outputs[0]))\n        \n        result.append({'true': a, 'predicted': ans})\n        \n        if examples:\n            print('Q: ' + q + '\\nTrue A: ' + a + '\\nPredicted A: ' + ans[0] + '\\n\\n')\n    \n    return result\n\n\ndef get_accuracy(result):\n    correct = 0\n    \n    for res in result:\n        true = res['true'].split(' ')\n        true = true[-1]\n        \n        preds = res['predicted']\n        \n        pred_list = []\n        for pred in preds:\n            ans = get_ans(pred)\n            if ans:\n                pred_list.append(ans)\n        if not pred_list:\n            continue\n        maj_ans = get_maj(pred_list)\n        if _is_float(true) and _is_float(maj_ans):\n            if abs(float(true) - float(maj_ans)) <= 1e-5:\n                correct += 1\n        elif str(true) == str(maj_ans):\n            correct += 1\n    \n    total = len(result)\n    return correct, total, correct/total","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:42:13.452867Z","iopub.execute_input":"2023-02-27T17:42:13.453904Z","iopub.status.idle":"2023-02-27T17:42:13.466491Z","shell.execute_reply.started":"2023-02-27T17:42:13.453853Z","shell.execute_reply":"2023-02-27T17:42:13.465511Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Получим результаты для жадного сэмплирования","metadata":{}},{"cell_type":"code","source":"result_default = get_predictions(data_test, PROMT, examples=True)\ncorrect, total, acc = get_accuracy(result_default)\nprint(\"Correct tasks:\", correct, \"Total tasks:\", total, \"Accuracy:\", acc)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T17:42:17.934734Z","iopub.execute_input":"2023-02-27T17:42:17.935097Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  0%|          | 0/20 [00:00<?, ?it/s]Feb 27 17:42:18.194 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1m/opt/conda/lib/python3.7/site-packages/petals/client/inference_session.py.step:312\u001b[0m] Caught exception when running inference from block 0 (retry in 0 sec): ControlFailure('Connect failed. msg=routing: not found')\nFeb 27 17:42:18.592 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1m/opt/conda/lib/python3.7/site-packages/petals/client/inference_session.py.step:312\u001b[0m] Caught exception when running inference from block 0 (retry in 1 sec): ControlFailure('Connect failed. msg=failed to find peers: routing: not found')\nFeb 27 17:42:19.993 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1m/opt/conda/lib/python3.7/site-packages/petals/client/inference_session.py.step:312\u001b[0m] Caught exception when running inference from block 0 (retry in 2 sec): ControlFailure('Connect failed. msg=failed to find peers: routing: not found')\nFeb 27 17:42:22.394 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1m/opt/conda/lib/python3.7/site-packages/petals/client/inference_session.py.step:312\u001b[0m] Caught exception when running inference from block 0 (retry in 4 sec): ControlFailure('Connect failed. msg=failed to find peers: routing: not found')\nFeb 27 17:42:26.796 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1m/opt/conda/lib/python3.7/site-packages/petals/client/inference_session.py.step:312\u001b[0m] Caught exception when running inference from block 0 (retry in 8 sec): ControlFailure('Connect failed. msg=failed to find peers: routing: not found')\nFeb 27 18:03:19.326 [\u001b[1m\u001b[34mINFO\u001b[0m] Peer 12D3KooWAp4g4yANgjAN6pZodKYCwqPq7p4jhSCYKaBCYLY3CwQX did not respond, banning it temporarily\nFeb 27 18:03:19.327 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1m/opt/conda/lib/python3.7/site-packages/petals/client/inference_session.py.step:312\u001b[0m] Caught exception when running inference from block 34 (retry in 0 sec): TimeoutError()\nFeb 27 18:03:19.330 [\u001b[1m\u001b[34mINFO\u001b[0m] Due to a server failure, remote attention caches from block 34 to 36 will be regenerated\nFeb 27 18:03:19.332 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1m/opt/conda/lib/python3.7/site-packages/petals/client/routing/sequence_manager.py.make_sequence:109\u001b[0m] Remote SequenceManager is still searching for routes, waiting for it to become ready\nFeb 27 18:20:48.433 [\u001b[1m\u001b[34mINFO\u001b[0m] Peer 12D3KooWFGDKjaCyYzcoGxuLvTwkdCzMePxShF7p2wNC8gE6rVNZ did not respond, banning it temporarily\nFeb 27 18:20:48.435 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1m/opt/conda/lib/python3.7/site-packages/petals/client/inference_session.py.step:312\u001b[0m] Caught exception when running inference from block 36 (retry in 0 sec): TimeoutError()\nFeb 27 18:20:48.440 [\u001b[1m\u001b[34mINFO\u001b[0m] Due to a server failure, remote attention caches from block 36 to 63 will be regenerated\nFeb 27 18:20:48.441 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1m/opt/conda/lib/python3.7/site-packages/petals/client/routing/sequence_manager.py.make_sequence:109\u001b[0m] Remote SequenceManager is still searching for routes, waiting for it to become ready\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Теперь результаты для Self-consistency метода","metadata":{}},{"cell_type":"code","source":"result_cons = get_predictions(data_test, PROMT, examples=True, max_tokens=100, \n                              self_cons=True, num_samples=10, temp=0.7, top_k=40)\ncorrect, total, acc = get_accuracy(result_cons)\nprint(\"Correct tasks:\", correct, \"Total tasks:\", total, \"Accuracy:\", acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Повторение экспериментов жадного сэмплирования для снижения влияния рандома\ncorrect_def = []\ntotal_def = []\nacc_def = []\nfor _ in range(5):\n    num_tasks = 20\n    inds = random.sample(range(0, len(data)), num_tasks)\n    data_test = [data[i] for i in inds]\n    \n    result_default = get_predictions(data_test, PROMT, examples=True)\n    correct, total, acc = get_accuracy(result_default)\n    \n    correct_def.append(correct)\n    total_def.append(total)\n    acc_def.append(acc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Correct tasks:\", correct_def.mean(), \"Total tasks:\", total_def.mean(), \"Accuracy:\", acc_def.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Повторение экспериментов self-consistency сэмплирования для снижения влияния рандома\ncorrect_cons = []\ntotal_cons = []\nacc_cons = []\nfor _ in range(5):\n    num_tasks = 20\n    inds = random.sample(range(0, len(data)), num_tasks)\n    data_test = [data[i] for i in inds]\n    \n    result_cons = get_predictions(data_test, PROMT, examples=True, max_tokens=100, \n                              self_cons=True, num_samples=10, temp=0.7, top_k=40)\n    correct, total, acc = get_accuracy(result_cons)\n    \n    correct_cons.append(correct)\n    total_cons.append(total)\n    acc_cons.append(acc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Correct tasks:\", correct_cons.mean(), \"Total tasks:\", total_cons.mean(), \"Accuracy:\", acc_cons.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Здесь предполагается вывод по качеству полученных ответов и сравнение с результатами в статье","metadata":{}},{"cell_type":"markdown","source":"Теперь можно потюнить температуру и топ-к и посмотреть как будет меняться качество. Также желательно потворить несколько раз на случайном наборе задач из датасета.","metadata":{}},{"cell_type":"code","source":"temp_list = [0.5, 0.7, 0.9]\ntop_k_list = [20, 40, 60]\nresult_list = []\n\nfor temp in temp_list:\n    for top_k in top_k_list:\n        correct_cons = []\n        total_cons = []\n        acc_cons = []\n        for _ in range(5):\n            num_tasks = 20\n            inds = random.sample(range(0, len(data)), num_tasks)\n            data_test = [data[i] for i in inds]\n\n            result_cons = get_predictions(data_test, PROMT, examples=True, max_tokens=100, \n                                      self_cons=True, num_samples=10, temp=temp, top_k=top_k)\n            correct, total, acc = get_accuracy(result_cons)\n\n            correct_cons.append(correct)\n            total_cons.append(total)\n            acc_cons.append(acc)\n        result_list.append((acc_cons.mean(), temp, top_k))\n        print(\"Correct tasks:\", correct_cons.mean(), \"Total tasks:\", total_cons.mean(), \"Accuracy:\", acc_cons.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_results = sorted(result_list)\nprint(\"Best parameters:\")\nprint(\"Mean accuracy:\", sorted_results[-1][0], \"Best temp:\", sorted_results[-1][1], \"Best top_k:\", sorted_results[-1][2])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ещё было бы интересно попробовать повторить зависимость: больше разных решений аггрегируется - больше вероятность получить верный ответ. Во второй статье был отмечен значительный рост качества, если агрегируется от 10 вариантов. Также тут можно попробовать более сложный метод агрегации, например Дэвида-Скина.","metadata":{}},{"cell_type":"markdown","source":"# Предложения для дальнейших экспериментов и улучшений качества:\n## 1. Суда по результатам из статьи полезно взять модель, у которой ещё больше параметров, чем у рассмотренной BLOOM-176B\n## 2. Попробовать поиграться с промтами: поискать более качественные, сэмплить случайные из датасета, создать свои\n## 3. Усложнить сэмплирование ответов, избавиться от приближенно хорошего Majority Vote.","metadata":{}}]}